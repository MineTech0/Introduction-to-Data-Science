{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Data Science 2025\n",
        "\n",
        "# Week 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this week's exercise, we look at prompting and zero- and few-shot task settings. Below is a text generation example from https://github.com/TurkuNLP/intro-to-nlp/blob/master/text_generation_pipeline_example.ipynb demonstrating how to load a text generation pipeline with a pre-trained model and generate text with a given prompt. Your task is to load a similar pre-trained generative model and assess whether the model succeeds at a set of tasks in zero-shot, one-shot, and two-shot settings.\n",
        "\n",
        "**Note: Downloading and running the pre-trained model locally may take some time. Alternatively, you can open and run this notebook on [Google Colab](https://colab.research.google.com/), as assumed in the following example.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIQ1s96UCcJW"
      },
      "source": [
        "## Text generation example\n",
        "\n",
        "This is a brief example of how to run text generation with a causal language model and `pipeline`.\n",
        "\n",
        "Install [transformers](https://huggingface.co/docs/transformers/index) python package. This will be used to load the model and tokenizer and to run generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4fUBJmXHCHw-"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZRNZgRJCt6Q"
      },
      "source": [
        "Import the `AutoTokenizer`, `AutoModelForCausalLM`, and `pipeline` classes. The first two support loading tokenizers and generative models from the [Hugging Face repository](https://huggingface.co/models), and the last wraps a tokenizer and a model for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jwyK005xCFSF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QJPDe3ZC_sL"
      },
      "source": [
        "Load a generative model and its tokenizer. You can substitute any other generative model name here (e.g. [other TurkuNLP GPT-3 models](https://huggingface.co/models?sort=downloads&search=turkunlp%2Fgpt3)), but note that Colab may have issues running larger models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wqTxn_QaCNjZ"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'TurkuNLP/gpt3-finnish-large'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ADWWb77e1sY"
      },
      "source": [
        "Instantiate a text generation pipeline using the tokenizer and model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0IIJzNrEe5qx"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\n",
        "    'text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=model.device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAohNr1ciwaU"
      },
      "source": [
        "We can now call the pipeline with a text prompt; it will take care of tokenizing, encoding, generation, and decoding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWcOJkiKi5vr",
        "outputId": "11cb09ab-310d-438e-e372-ef8d7f8f66a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'Terve, miten menee?”\\n”Hyvin, kiitos.”\\n”Kiva kuulla.”\\n”Kuule, minulla on sinulle asiaa.”\\n'}]\n"
          ]
        }
      ],
      "source": [
        "output = pipe('Terve, miten menee?', max_new_tokens=25)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNRMsxXOjSo0"
      },
      "source": [
        "Just print the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Op7MJ6XjahG",
        "outputId": "5d9e26dd-3e80-4663-a712-d85093fad073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terve, miten menee?”\n",
            "”Hyvin, kiitos.”\n",
            "”Kiva kuulla.”\n",
            "”Kuule, minulla on sinulle asiaa.”\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YROp3hyikXPO"
      },
      "source": [
        "We can also call the pipeline with any arguments that the model `generate` function supports. For details on text generation using `transformers`, see e.g. [this tutorial](https://huggingface.co/blog/how-to-generate).\n",
        "\n",
        "Example with sampling and a high `temperature` parameter to generate more chaotic output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22QjXE88jkim",
        "outputId": "372a4fc2-e305-4034-cea3-a52b6103c24f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terve, miten menee? kysyi Heikki yhtäkkiä astuessaan taloon sisään kantaen kahvipöytä-tarvikkeita käsissään sisään tultuaan.\n",
            "(Ryökäle luuli varmasti hänen tulevan meille istumaan)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "output = pipe(\n",
        "    'Terve, miten menee?',\n",
        "    do_sample=True,\n",
        "    temperature=10.0,\n",
        "    max_new_tokens=25\n",
        ")\n",
        "\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1\n",
        "\n",
        "Your task is to assess whether a generative model succeeds in the following tasks in zero-shot, one-shot, and two-shot settings:\n",
        "\n",
        "- binary sentiment classification (positive / negative)\n",
        "\n",
        "- person name recognition\n",
        "\n",
        "- two-digit addition (e.g. 11 + 22 = 33)\n",
        "\n",
        "For example, for assessing whether a generative model can name capital cities, we could use the following prompts:\n",
        "\n",
        "- zero-shot:\n",
        "\t>\"\"\"\\\n",
        "\t>Identify the capital cities of countries.\n",
        "\t>\n",
        "\t>Question: What is the capital of Finland?\\\n",
        "\t>Answer:\\\n",
        "\t>\"\"\"\n",
        "- one-shot:\n",
        "\t>\"\"\"\\\n",
        "\t>Identify the capital cities of countries.\n",
        "\t>\n",
        "\t>Question: What is the capital of Sweden?\\\n",
        "\t>Answer: Stockholm\n",
        "\t>\n",
        "\t>Question: What is the capital of Finland?\\\n",
        "\t>Answer:\\\n",
        "\t>\"\"\"\n",
        "- two-shot:\n",
        "\t>\"\"\"\\\n",
        "\t>Identify the capital cities of countries.\n",
        "\t>\n",
        "\t>Question: What is the capital of Sweden?\\\n",
        "\t>Answer: Stockholm\n",
        "\t>\n",
        "\t>Question: What is the capital of Denmark?\\\n",
        "\t>Answer: Copenhagen\n",
        "\t>\n",
        "\t>Question: What is the capital of Finland?\\\n",
        "\t>Answer:\\\n",
        "\t>\"\"\"\n",
        "\n",
        "You can do the tasks either in English or Finnish and use a generative model of your choice from the Hugging Face models repository, for example the following models:\n",
        "\n",
        "- English: `gpt2-large`\n",
        "- Finnish: `TurkuNLP/gpt3-finnish-large`\n",
        "\n",
        "You can either come up with your own instructions for the tasks or use the following:\n",
        "\n",
        "- English:\n",
        "\t- binary sentiment classification: \"Do the following texts express a positive or negative sentiment?\"\n",
        "\t- person name recognition: \"List the person names occurring in the following texts.\"\n",
        "\t- two-digit addition: \"This is a first grade math exam.\"\n",
        "- Finnish:\n",
        "\t- binary sentiment classification: \"Ilmaisevatko seuraavat tekstit positiivista vai negatiivista tunnetta?\"\n",
        "\t- person name recognition: \"Listaa seuraavissa teksteissä mainitut henkilönnimet.\"\n",
        "\t- two-digit addition: \"Tämä on ensimmäisen luokan matematiikan koe.\"\n",
        "\n",
        "Come up with at least two test cases for each of the three tasks, and come up with your own one- and two-shot examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Load model\n",
        "MODEL_NAME = 'Qwen/Qwen2.5-1.5B-Instruct'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "pipe = pipeline(\n",
        "    'text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device='cpu'\n",
        ")\n",
        "\n",
        "def generate_response(prompt, max_tokens=10):\n",
        "    output = pipe(prompt, max_new_tokens=max_tokens, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "    response = output[0]['generated_text'][len(prompt):].strip()\n",
        "    \n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SENTIMENT CLASSIFICATION ===\n",
            "Zero-shot:\n",
            "positive\n",
            "\n",
            "The sentiment expressed in the given text is\n",
            "\n",
            "One-shot:\n",
            "positive\n",
            "\n",
            "The sentiment expressed in the given text is\n",
            "\n",
            "One-shot:\n",
            "positive\n",
            "\n",
            "Text: The weather is so nice today\n",
            "\n",
            "Two-shot:\n",
            "positive\n",
            "\n",
            "Text: The weather is so nice today\n",
            "\n",
            "Two-shot:\n",
            "positive\n",
            "\n",
            "Text: The food at this restaurant is\n",
            "positive\n",
            "\n",
            "Text: The food at this restaurant is\n"
          ]
        }
      ],
      "source": [
        "# Task 1: Binary Sentiment Classification\n",
        "print(\"=== SENTIMENT CLASSIFICATION ===\")\n",
        "\n",
        "# First test case\n",
        "zero_shot_sentiment = \"\"\"Classify the sentiment of the following text as either 'positive' or 'negative'. Only respond with one word.\n",
        "\n",
        "Text: This movie was absolutely amazing and delightful!\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "print(\"Zero-shot:\")\n",
        "print(generate_response(zero_shot_sentiment))\n",
        "\n",
        "one_shot_sentiment = \"\"\"Classify the sentiment of the following text as either 'positive' or 'negative'. Only respond with one word.\n",
        "\n",
        "Text: I hate this terrible product.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: This movie was absolutely amazing and delightful!\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "print(\"\\nOne-shot:\")\n",
        "print(generate_response(one_shot_sentiment))\n",
        "\n",
        "two_shot_sentiment = \"\"\"Classify the sentiment of the following text as either 'positive' or 'negative'. Only respond with one word.\n",
        "\n",
        "Text: I hate this terrible product.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: This book is wonderful and inspiring.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: This movie was absolutely amazing and delightful!\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "print(\"\\nTwo-shot:\")\n",
        "print(generate_response(two_shot_sentiment))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Second sentiment test ---\n",
            "Zero-shot:\n",
            "negative.\n",
            "\n",
            "One-shot:\n",
            "negative.\n",
            "\n",
            "One-shot:\n",
            "negative\n",
            "\n",
            "Two-shot:\n",
            "negative\n",
            "\n",
            "Two-shot:\n",
            "negative\n",
            "negative\n"
          ]
        }
      ],
      "source": [
        "# Second sentiment test case\n",
        "print(\"\\n--- Second sentiment test ---\")\n",
        "\n",
        "zero_shot_sentiment2 = \"\"\"Classify the sentiment of the following text as either 'positive' or 'negative'. Only respond with one word.\n",
        "\n",
        "Text: The weather is horrible and ruined my day.\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "print(\"Zero-shot:\")\n",
        "print(generate_response(zero_shot_sentiment2))\n",
        "\n",
        "one_shot_sentiment2 = \"\"\"Classify the sentiment of the following text as either 'positive' or 'negative'. Only respond with one word.\n",
        "\n",
        "Text: I hate this terrible product.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The weather is horrible and ruined my day.\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "print(\"\\nOne-shot:\")\n",
        "print(generate_response(one_shot_sentiment2))\n",
        "\n",
        "two_shot_sentiment2 = \"\"\"Classify the sentiment of the following text as either 'positive' or 'negative'. Only respond with one word.\n",
        "\n",
        "Text: I hate this terrible product.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: This book is wonderful and inspiring.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: The weather is horrible and ruined my day.\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "print(\"\\nTwo-shot:\")\n",
        "print(generate_response(two_shot_sentiment2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== PERSON NAME RECOGNITION ===\n",
            "Zero-shot:\n",
            "['John', 'Smith', 'Sarah', '\n",
            "\n",
            "One-shot:\n",
            "['John', 'Smith', 'Sarah', '\n",
            "\n",
            "One-shot:\n",
            "John Smith, Sarah Johnson\n",
            "\n",
            "Two-shot:\n",
            "John Smith, Sarah Johnson\n",
            "\n",
            "Two-shot:\n",
            "John Smith, Sarah Johnson\n",
            "John Smith, Sarah Johnson\n"
          ]
        }
      ],
      "source": [
        "# Task 2: Person Name Recognition\n",
        "print(\"=== PERSON NAME RECOGNITION ===\")\n",
        "\n",
        "# First test case\n",
        "zero_shot_names = \"\"\"List the person names occurring in the following texts.\n",
        "\n",
        "Text: John Smith met with Sarah Johnson at the conference yesterday.\n",
        "Names:\"\"\"\n",
        "\n",
        "print(\"Zero-shot:\")\n",
        "print(generate_response(zero_shot_names))\n",
        "\n",
        "one_shot_names = \"\"\"List the person names occurring in the following texts.\n",
        "\n",
        "Text: Michael Brown and Lisa Davis went to the store.\n",
        "Names: Michael Brown, Lisa Davis\n",
        "\n",
        "Text: John Smith met with Sarah Johnson at the conference yesterday.\n",
        "Names:\"\"\"\n",
        "\n",
        "print(\"\\nOne-shot:\")\n",
        "print(generate_response(one_shot_names))\n",
        "\n",
        "two_shot_names = \"\"\"List the person names occurring in the following texts.\n",
        "\n",
        "Text: Michael Brown and Lisa Davis went to the store.\n",
        "Names: Michael Brown, Lisa Davis\n",
        "\n",
        "Text: Professor Wilson taught the class while Emma Thompson took notes.\n",
        "Names: Wilson, Emma Thompson\n",
        "\n",
        "Text: John Smith met with Sarah Johnson at the conference yesterday.\n",
        "Names:\"\"\"\n",
        "\n",
        "print(\"\\nTwo-shot:\")\n",
        "print(generate_response(two_shot_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Second name recognition test ---\n",
            "Zero-shot:\n",
            "Dr. Anderson, Maria Garcia\n",
            "\n",
            "One-shot:\n",
            "Dr. Anderson, Maria Garcia\n",
            "\n",
            "One-shot:\n",
            "Dr. Anderson, Maria Garcia\n",
            "\n",
            "Two-shot:\n",
            "Dr. Anderson, Maria Garcia\n",
            "\n",
            "Two-shot:\n",
            "Anderson, Maria Garcia\n",
            "Anderson, Maria Garcia\n"
          ]
        }
      ],
      "source": [
        "# Second name recognition test case\n",
        "print(\"\\n--- Second name recognition test ---\")\n",
        "\n",
        "zero_shot_names2 = \"\"\"List the person names occurring in the following texts.\n",
        "\n",
        "Text: Dr. Anderson spoke with Maria Garcia about the research project.\n",
        "Names:\"\"\"\n",
        "\n",
        "print(\"Zero-shot:\")\n",
        "print(generate_response(zero_shot_names2))\n",
        "\n",
        "one_shot_names2 = \"\"\"List the person names occurring in the following texts.\n",
        "\n",
        "Text: Michael Brown and Lisa Davis went to the store.\n",
        "Names: Michael Brown, Lisa Davis\n",
        "\n",
        "Text: Dr. Anderson spoke with Maria Garcia about the research project.\n",
        "Names:\"\"\"\n",
        "\n",
        "print(\"\\nOne-shot:\")\n",
        "print(generate_response(one_shot_names2))\n",
        "\n",
        "two_shot_names2 = \"\"\"List the person names occurring in the following texts.\n",
        "\n",
        "Text: Michael Brown and Lisa Davis went to the store.\n",
        "Names: Michael Brown, Lisa Davis\n",
        "\n",
        "Text: Professor Wilson taught the class while Emma Thompson took notes.\n",
        "Names: Wilson, Emma Thompson\n",
        "\n",
        "Text: Dr. Anderson spoke with Maria Garcia about the research project.\n",
        "Names:\"\"\"\n",
        "\n",
        "print(\"\\nTwo-shot:\")\n",
        "print(generate_response(two_shot_names2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== TWO-DIGIT ADDITION ===\n",
            "Zero-shot:\n",
            "68\n",
            "\n",
            "What are the steps to solve\n",
            "\n",
            "One-shot:\n",
            "68\n",
            "\n",
            "What are the steps to solve\n",
            "\n",
            "One-shot:\n",
            "68\n",
            "\n",
            "Problem: 78 -\n",
            "\n",
            "Two-shot:\n",
            "68\n",
            "\n",
            "Problem: 78 -\n",
            "\n",
            "Two-shot:\n",
            "68\n",
            "\n",
            "What is the next problem in\n",
            "68\n",
            "\n",
            "What is the next problem in\n"
          ]
        }
      ],
      "source": [
        "# Task 3: Two-digit Addition\n",
        "print(\"=== TWO-DIGIT ADDITION ===\")\n",
        "\n",
        "# First test case\n",
        "zero_shot_math = \"\"\"This is a first grade math exam.\n",
        "\n",
        "Problem: 23 + 45 = \n",
        "Answer:\"\"\"\n",
        "\n",
        "print(\"Zero-shot:\")\n",
        "print(generate_response(zero_shot_math))\n",
        "\n",
        "one_shot_math = \"\"\"This is a first grade math exam.\n",
        "\n",
        "Problem: 12 + 34 = 46\n",
        "\n",
        "Problem: 23 + 45 = \n",
        "Answer:\"\"\"\n",
        "\n",
        "print(\"\\nOne-shot:\")\n",
        "print(generate_response(one_shot_math))\n",
        "\n",
        "two_shot_math = \"\"\"This is a first grade math exam.\n",
        "\n",
        "Problem: 12 + 34 = 46\n",
        "Problem: 67 + 21 = 88\n",
        "\n",
        "Problem: 23 + 45 = \n",
        "Answer:\"\"\"\n",
        "\n",
        "print(\"\\nTwo-shot:\")\n",
        "print(generate_response(two_shot_math))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Second math test ---\n",
            "Zero-shot:\n",
            "93\n",
            "\n",
            "What are the steps to solve\n",
            "\n",
            "One-shot:\n",
            "93\n",
            "\n",
            "What are the steps to solve\n",
            "\n",
            "One-shot:\n",
            "93\n",
            "\n",
            "Problem: 89 +\n",
            "\n",
            "Two-shot:\n",
            "93\n",
            "\n",
            "Problem: 89 +\n",
            "\n",
            "Two-shot:\n",
            "93\n",
            "\n",
            "What is the next problem in\n",
            "93\n",
            "\n",
            "What is the next problem in\n"
          ]
        }
      ],
      "source": [
        "# Second math test case\n",
        "print(\"\\n--- Second math test ---\")\n",
        "\n",
        "zero_shot_math2 = \"\"\"This is a first grade math exam.\n",
        "\n",
        "Problem: 56 + 37 = \n",
        "Answer:\"\"\"\n",
        "\n",
        "print(\"Zero-shot:\")\n",
        "print(generate_response(zero_shot_math2))\n",
        "\n",
        "one_shot_math2 = \"\"\"This is a first grade math exam.\n",
        "\n",
        "Problem: 12 + 34 = 46\n",
        "\n",
        "Problem: 56 + 37 = \n",
        "Answer:\"\"\"\n",
        "\n",
        "print(\"\\nOne-shot:\")\n",
        "print(generate_response(one_shot_math2))\n",
        "\n",
        "two_shot_math2 = \"\"\"This is a first grade math exam.\n",
        "\n",
        "Problem: 12 + 34 = 46\n",
        "Problem: 67 + 21 = 88\n",
        "\n",
        "Problem: 56 + 37 = \n",
        "Answer:\"\"\"\n",
        "\n",
        "print(\"\\nTwo-shot:\")\n",
        "print(generate_response(two_shot_math2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Submit this exercise by submitting your code and your answers to the above questions as comments on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMPTKW2dgboQJpXpHYIBCHp",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
